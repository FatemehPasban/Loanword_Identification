{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Root Finder Model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOmVc8cpWmGsBDlvf+TWdAc",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "609ec6e3f7784092a53d4d35ee8429a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_93d09d973c004c3598acf0546eae2803",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_61f734a5ad184d9a87703c3e9fe5c4e0",
              "IPY_MODEL_3c2572fb06e74a19a3a2199fb33c276a",
              "IPY_MODEL_6349af49a30a4515b42878508c543093"
            ]
          }
        },
        "93d09d973c004c3598acf0546eae2803": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "61f734a5ad184d9a87703c3e9fe5c4e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_43c5ff2d772a40899e87a1921ce2d738",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f21ad9ff8227471f92824efd08f8946d"
          }
        },
        "3c2572fb06e74a19a3a2199fb33c276a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_883f4627f996435a9c2754164075600a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1198122,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1198122,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7dc05edc6de74b1894131a405b001fa7"
          }
        },
        "6349af49a30a4515b42878508c543093": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2f6963d653114b6691a955c55ad2750f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.20M/1.20M [00:00&lt;00:00, 1.63MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b37afae588c24729bf07f389f3cf94eb"
          }
        },
        "43c5ff2d772a40899e87a1921ce2d738": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f21ad9ff8227471f92824efd08f8946d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "883f4627f996435a9c2754164075600a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7dc05edc6de74b1894131a405b001fa7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2f6963d653114b6691a955c55ad2750f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b37afae588c24729bf07f389f3cf94eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ea0becabd5324f9b869cb899fd8c71c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1b92f5f7c1784fa6b273d5ed0a1b850c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_03d323e4a8a84f549176c0ae23c41d47",
              "IPY_MODEL_551381a2bd6c4985a596a72dfdfa3b86",
              "IPY_MODEL_d5e2a13fb2414ddeaf6563f06d8ffd4a"
            ]
          }
        },
        "1b92f5f7c1784fa6b273d5ed0a1b850c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "03d323e4a8a84f549176c0ae23c41d47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_729e124896354ca48fe5aeb4966cd9fc",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ca0b149564704d6ba9f65d4b6788f80a"
          }
        },
        "551381a2bd6c4985a596a72dfdfa3b86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_fd9a1524a4e34f3280ea4ce5fb6e75c5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 440,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 440,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b5eda4da27414755a569160a83ff77b6"
          }
        },
        "d5e2a13fb2414ddeaf6563f06d8ffd4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_aaf1db02fb10406ea65f8a1021843764",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 440/440 [00:00&lt;00:00, 8.33kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b8cd5e1e0032401f868e44ce430272e8"
          }
        },
        "729e124896354ca48fe5aeb4966cd9fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ca0b149564704d6ba9f65d4b6788f80a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fd9a1524a4e34f3280ea4ce5fb6e75c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b5eda4da27414755a569160a83ff77b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "aaf1db02fb10406ea65f8a1021843764": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b8cd5e1e0032401f868e44ce430272e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FatemehPasban/Loanword_Identification/blob/main/Root_Finder_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykd8xX30jcno",
        "outputId": "5fce867a-c05f-4bd3-974b-2d409a67df74"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4XsiPuEqE2A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "041b75ad-7e49-4342-b4fb-8e47210b3809"
      },
      "source": [
        "!pip install hazm\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hazm\n",
            "  Downloading hazm-0.7.0-py3-none-any.whl (316 kB)\n",
            "\u001b[K     |████████████████████████████████| 316 kB 5.2 MB/s \n",
            "\u001b[?25hCollecting nltk==3.3\n",
            "  Downloading nltk-3.3.0.zip (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 48.0 MB/s \n",
            "\u001b[?25hCollecting libwapiti>=0.2.1\n",
            "  Downloading libwapiti-0.2.1.tar.gz (233 kB)\n",
            "\u001b[K     |████████████████████████████████| 233 kB 49.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.3->hazm) (1.15.0)\n",
            "Building wheels for collected packages: nltk, libwapiti\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.3-py3-none-any.whl size=1394483 sha256=1ca792655713a900aedcc2824a8957dcb58fba61e37ef0cedf6839e112dc1168\n",
            "  Stored in directory: /root/.cache/pip/wheels/9b/fd/0c/d92302c876e5de87ebd7fc0979d82edb93e2d8d768bf71fac4\n",
            "  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for libwapiti: filename=libwapiti-0.2.1-cp37-cp37m-linux_x86_64.whl size=154389 sha256=80d664894038902e6ce750b936a322536d522d7262a06ae9aa43612f6d2f41b7\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/b2/5b/0fe4b8f5c0e65341e8ea7bb3f4a6ebabfe8b1ac31322392dbf\n",
            "Successfully built nltk libwapiti\n",
            "Installing collected packages: nltk, libwapiti, hazm\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed hazm-0.7.0 libwapiti-0.2.1 nltk-3.3\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.10.3-py3-none-any.whl (2.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 55.8 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub>=0.0.12\n",
            "  Downloading huggingface_hub-0.0.17-py3-none-any.whl (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 29.0 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 40.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.17 pyyaml-5.4.1 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.10.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSgK3YgOmWN5"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from os.path import join\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "from hazm import *\n",
        "from transformers import BertConfig, BertTokenizer,BertModel,AutoTokenizer\n",
        "\n",
        "import torch\n",
        "import torch.utils.data as data_utils\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import precision_score\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIGdWVEzyv5o"
      },
      "source": [
        "## config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2P7CBUsOyvXG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "609ec6e3f7784092a53d4d35ee8429a4",
            "93d09d973c004c3598acf0546eae2803",
            "61f734a5ad184d9a87703c3e9fe5c4e0",
            "3c2572fb06e74a19a3a2199fb33c276a",
            "6349af49a30a4515b42878508c543093",
            "43c5ff2d772a40899e87a1921ce2d738",
            "f21ad9ff8227471f92824efd08f8946d",
            "883f4627f996435a9c2754164075600a",
            "7dc05edc6de74b1894131a405b001fa7",
            "2f6963d653114b6691a955c55ad2750f",
            "b37afae588c24729bf07f389f3cf94eb",
            "ea0becabd5324f9b869cb899fd8c71c3",
            "1b92f5f7c1784fa6b273d5ed0a1b850c",
            "03d323e4a8a84f549176c0ae23c41d47",
            "551381a2bd6c4985a596a72dfdfa3b86",
            "d5e2a13fb2414ddeaf6563f06d8ffd4a",
            "729e124896354ca48fe5aeb4966cd9fc",
            "ca0b149564704d6ba9f65d4b6788f80a",
            "fd9a1524a4e34f3280ea4ce5fb6e75c5",
            "b5eda4da27414755a569160a83ff77b6",
            "aaf1db02fb10406ea65f8a1021843764",
            "b8cd5e1e0032401f868e44ce430272e8"
          ]
        },
        "outputId": "51ef498e-533a-4ec2-a07a-80d366b0ff98"
      },
      "source": [
        "#model_name = 'HooshvareLab/bert-fa-zwnj-base'\n",
        "#model_name = 'HooshvareLab/distilbert-fa-zwnj-base'\n",
        "#model_name = 'HooshvareLab/albert-fa-zwnj-base-v2'\n",
        "model_name = 'HooshvareLab/bert-fa-base-uncased'\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "\n",
        "root_path ='/content/drive/MyDrive/uni2_project/Root Finder Model/'\n",
        "data_path=join(root_path,'words.xlsx')\n",
        "#data_path=join(root_path,'word2.xlsx')\n",
        "config = BertConfig.from_pretrained(\n",
        "    model_name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "609ec6e3f7784092a53d4d35ee8429a4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.20M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ea0becabd5324f9b869cb899fd8c71c3",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/440 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q715rL7wkRR1"
      },
      "source": [
        "# The data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        },
        "id": "mlvlhNpSkEg3",
        "outputId": "ca2a017d-5870-447c-9437-c5bd6480948b"
      },
      "source": [
        "raw_data = pd.read_excel(data_path)\n",
        "raw_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>syn1</th>\n",
              "      <th>syn2</th>\n",
              "      <th>syn3</th>\n",
              "      <th>syn4</th>\n",
              "      <th>syn5</th>\n",
              "      <th>syn6</th>\n",
              "      <th>syn7</th>\n",
              "      <th>syn8</th>\n",
              "      <th>syn9</th>\n",
              "      <th>syn10</th>\n",
              "      <th>syn11</th>\n",
              "      <th>syn12</th>\n",
              "      <th>syn13</th>\n",
              "      <th>syn14</th>\n",
              "      <th>syn15</th>\n",
              "      <th>syn16</th>\n",
              "      <th>syn17</th>\n",
              "      <th>Unnamed: 18</th>\n",
              "      <th>Unnamed: 19</th>\n",
              "      <th>Unnamed: 20</th>\n",
              "      <th>Unnamed: 21</th>\n",
              "      <th>Unnamed: 22</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>آب جاری</td>\n",
              "      <td>رواناب</td>\n",
              "      <td>آب روان</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>آباء و اجداد</td>\n",
              "      <td>پدران</td>\n",
              "      <td>نیاکان</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>آتیه</td>\n",
              "      <td>آینده</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>آثار</td>\n",
              "      <td>نشانه‌ها</td>\n",
              "      <td>نوشته‌ها</td>\n",
              "      <td>کارها</td>\n",
              "      <td>ردپا</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>آحاد</td>\n",
              "      <td>یکایک</td>\n",
              "      <td>تک تک</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4200</th>\n",
              "      <td>یأس</td>\n",
              "      <td>ناامیدی</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4201</th>\n",
              "      <td>یبوست</td>\n",
              "      <td>خشکی</td>\n",
              "      <td>دیرگواری</td>\n",
              "      <td>سخت گواری</td>\n",
              "      <td>بدگواری</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4202</th>\n",
              "      <td>یحیی</td>\n",
              "      <td>می‌زید</td>\n",
              "      <td>زنده است</td>\n",
              "      <td>زندگی می‌کند</td>\n",
              "      <td>نامی پسرانه</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4203</th>\n",
              "      <td>یعنی</td>\n",
              "      <td>یا اینکه</td>\n",
              "      <td>روشنتر آنکه</td>\n",
              "      <td>برابر با اینکه</td>\n",
              "      <td>یا می‌گوییم</td>\n",
              "      <td>به زبان دیگر</td>\n",
              "      <td>به سخن دیگر</td>\n",
              "      <td>به گفته دیگر</td>\n",
              "      <td>یا می‌توان گفت</td>\n",
              "      <td>یا همانکه</td>\n",
              "      <td>یا آنکه (از ستاک ساختگی سه گانی برگرفته از مع...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4204</th>\n",
              "      <td>یقین</td>\n",
              "      <td>باور</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4205 rows × 23 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                  word       syn1  ... Unnamed: 21 Unnamed: 22\n",
              "0             آب جاری      رواناب  ...         NaN         NaN\n",
              "1        آباء و اجداد       پدران  ...         NaN         NaN\n",
              "2                آتیه       آینده  ...         NaN         NaN\n",
              "3                آثار    نشانه‌ها  ...         NaN         NaN\n",
              "4                آحاد       یکایک  ...         NaN         NaN\n",
              "...                ...        ...  ...         ...         ...\n",
              "4200              یأس     ناامیدی  ...         NaN         NaN\n",
              "4201            یبوست        خشکی  ...         NaN         NaN\n",
              "4202             یحیی      می‌زید  ...         NaN         NaN\n",
              "4203             یعنی    یا اینکه  ...         NaN         NaN\n",
              "4204             یقین        باور  ...         NaN         NaN\n",
              "\n",
              "[4205 rows x 23 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efcfZ7eonSwB",
        "outputId": "8d499154-2d02-483a-9a54-f85e6b86ba73"
      },
      "source": [
        "def parse_data(row):\n",
        "  data_list =[]\n",
        "  row_val = row.values\n",
        "  Syns = row_val[0]\n",
        "  data_list.append({'word' : list( Syns[0].strip()) ,'label' : 1})\n",
        "  for syn_word in Syns[1:]:\n",
        "    if type(syn_word) is str:\n",
        "      syn_word = syn_word.strip()\n",
        "      data_list.append({'word' : list(syn_word),'label' : 0})\n",
        "  return data_list\n",
        "\n",
        "a = parse_data(raw_data[1:2])\n",
        "a"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 1,\n",
              "  'word': ['آ', 'ب', 'ا', 'ء', ' ', 'و', ' ', 'ا', 'ج', 'د', 'ا', 'د']},\n",
              " {'label': 0, 'word': ['پ', 'د', 'ر', 'ا', 'ن']},\n",
              " {'label': 0, 'word': ['ن', 'ی', 'ا', 'ک', 'ا', 'ن']}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "pJzpmL7-9P_p",
        "outputId": "6ca40f8d-d029-47b1-e02e-2b769e44533b"
      },
      "source": [
        "\n",
        "parsed = [parse_data(raw_data[i:i+1]) for i in range(len(raw_data))] # nested list => [ [{},{}] , [{}] , [{},{},{}] , ...]\n",
        "parsed = [i for item in parsed for i in item] # merging nested list => [ {} , {} , {} , ... ]\n",
        "df_data = pd.DataFrame(parsed)\n",
        "df_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[آ, ب,  , ج, ا, ر, ی]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[ر, و, ا, ن, ا, ب]</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[آ, ب,  , ر, و, ا, ن]</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[آ, ب, ا, ء,  , و,  , ا, ج, د, ا, د]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[پ, د, ر, ا, ن]</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15756</th>\n",
              "      <td>[ی, ا,  , م, ی, ‌, ت, و, ا, ن,  , گ, ف, ت]</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15757</th>\n",
              "      <td>[ی, ا,  , ه, م, ا, ن, ک, ه]</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15758</th>\n",
              "      <td>[ی, ا,  , آ, ن, ک, ه,  , (, ا, ز,  , س, ت, ا, ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15759</th>\n",
              "      <td>[ی, ق, ی, ن]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15760</th>\n",
              "      <td>[ب, ا, و, ر]</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>15761 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    word  label\n",
              "0                                  [آ, ب,  , ج, ا, ر, ی]      1\n",
              "1                                     [ر, و, ا, ن, ا, ب]      0\n",
              "2                                  [آ, ب,  , ر, و, ا, ن]      0\n",
              "3                   [آ, ب, ا, ء,  , و,  , ا, ج, د, ا, د]      1\n",
              "4                                        [پ, د, ر, ا, ن]      0\n",
              "...                                                  ...    ...\n",
              "15756         [ی, ا,  , م, ی, ‌, ت, و, ا, ن,  , گ, ف, ت]      0\n",
              "15757                        [ی, ا,  , ه, م, ا, ن, ک, ه]      0\n",
              "15758  [ی, ا,  , آ, ن, ک, ه,  , (, ا, ز,  , س, ت, ا, ...      0\n",
              "15759                                       [ی, ق, ی, ن]      1\n",
              "15760                                       [ب, ا, و, ر]      0\n",
              "\n",
              "[15761 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "12BQldmPilt_",
        "outputId": "67909588-4c3d-4851-b454-512d30ff9cfe"
      },
      "source": [
        "df_data.groupby('label').count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>label</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>11556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4205</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        word\n",
              "label       \n",
              "0      11556\n",
              "1       4205"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5TbR1XLcp_i"
      },
      "source": [
        "### Equalize data in the direction of the label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "jVV1ssDnAYB1",
        "outputId": "50e56c04-bb6d-4e53-f558-1d09d5b12c07"
      },
      "source": [
        "df_dataa = shuffle(df_data)\n",
        "df_data0 = df_dataa[df_dataa.label == 0]\n",
        "df_data1 = df_dataa[df_dataa.label == 1]\n",
        "\n",
        "df_data_new = shuffle( pd.concat ([df_data0[:len(df_data1)] , df_data1] , axis=0) )\n",
        "#df_data_new = pd.concat ([ df_data0[:len(df_data1)] , df_data1 ]) \n",
        "df_data_new"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6463</th>\n",
              "      <td>[خ, ن, ث, ی]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4758</th>\n",
              "      <td>[ت, م, س, ا, ح]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6501</th>\n",
              "      <td>[خ, ی, ا, ل,  , پ, ر, د, ا, ز, ا, ن, ه]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2491</th>\n",
              "      <td>[ب, د,  , ذ, ا, ت]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14262</th>\n",
              "      <td>[ه, م, س, ا, ز, ی]</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10299</th>\n",
              "      <td>[ف, ا, ر, غ, ‌, ا, ل, ت, ح, ص, ی, ل]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8473</th>\n",
              "      <td>[ص, ن, ع, ت]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1766</th>\n",
              "      <td>[پ, و, ش, ا, ک]</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11765</th>\n",
              "      <td>[و, ر, ج, ا, و, ن, د]</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14801</th>\n",
              "      <td>[ب, ی, ‌, آ, ب, ر, و]</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8410 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          word  label\n",
              "6463                              [خ, ن, ث, ی]      1\n",
              "4758                           [ت, م, س, ا, ح]      1\n",
              "6501   [خ, ی, ا, ل,  , پ, ر, د, ا, ز, ا, ن, ه]      1\n",
              "2491                        [ب, د,  , ذ, ا, ت]      1\n",
              "14262                       [ه, م, س, ا, ز, ی]      0\n",
              "...                                        ...    ...\n",
              "10299     [ف, ا, ر, غ, ‌, ا, ل, ت, ح, ص, ی, ل]      1\n",
              "8473                              [ص, ن, ع, ت]      1\n",
              "1766                           [پ, و, ش, ا, ک]      0\n",
              "11765                    [و, ر, ج, ا, و, ن, د]      0\n",
              "14801                    [ب, ی, ‌, آ, ب, ر, و]      0\n",
              "\n",
              "[8410 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "XNAlCQXOZb9j",
        "outputId": "48e9b27e-7f6a-4806-9b1c-4e780465890a"
      },
      "source": [
        "df_data_new.groupby('label').count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>label</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4205</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4205</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       word\n",
              "label      \n",
              "0      4205\n",
              "1      4205"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynqBW9Bkol9k"
      },
      "source": [
        "## Encoding the Dataset with the Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "I-97K5gKuJzc",
        "outputId": "9cd4032a-fbb9-483f-9514-78d3ba28182b"
      },
      "source": [
        "encode_words = [tokenizer.encode(text)\n",
        "                for text in df_data_new['word']]\n",
        "df_data_new['encoded_words'] = encode_words\n",
        "df_data_new\n",
        "\n",
        "#Some time it has\n",
        "#ValueError: Input [] is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>label</th>\n",
              "      <th>encoded_words</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6463</th>\n",
              "      <td>[خ, ن, ث, ی]</td>\n",
              "      <td>1</td>\n",
              "      <td>[2, 1359, 1377, 1356, 1442, 4]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4758</th>\n",
              "      <td>[ت, م, س, ا, ح]</td>\n",
              "      <td>1</td>\n",
              "      <td>[2, 1355, 1376, 1364, 1352, 1358, 4]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6501</th>\n",
              "      <td>[خ, ی, ا, ل,  , پ, ر, د, ا, ز, ا, ن, ه]</td>\n",
              "      <td>1</td>\n",
              "      <td>[2, 1359, 1442, 1352, 1375, 1, 1407, 1362, 136...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2491</th>\n",
              "      <td>[ب, د,  , ذ, ا, ت]</td>\n",
              "      <td>1</td>\n",
              "      <td>[2, 1353, 1360, 1, 1361, 1352, 1355, 4]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14262</th>\n",
              "      <td>[ه, م, س, ا, ز, ی]</td>\n",
              "      <td>0</td>\n",
              "      <td>[2, 1378, 1376, 1364, 1352, 1363, 1442, 4]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10299</th>\n",
              "      <td>[ف, ا, ر, غ, ‌, ا, ل, ت, ح, ص, ی, ل]</td>\n",
              "      <td>1</td>\n",
              "      <td>[2, 1373, 1352, 1362, 1371, 1, 1352, 1375, 135...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8473</th>\n",
              "      <td>[ص, ن, ع, ت]</td>\n",
              "      <td>1</td>\n",
              "      <td>[2, 1366, 1377, 1370, 1355, 4]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1766</th>\n",
              "      <td>[پ, و, ش, ا, ک]</td>\n",
              "      <td>0</td>\n",
              "      <td>[2, 1407, 1379, 1365, 1352, 1426, 4]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11765</th>\n",
              "      <td>[و, ر, ج, ا, و, ن, د]</td>\n",
              "      <td>0</td>\n",
              "      <td>[2, 1379, 1362, 1357, 1352, 1379, 1377, 1360, 4]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14801</th>\n",
              "      <td>[ب, ی, ‌, آ, ب, ر, و]</td>\n",
              "      <td>0</td>\n",
              "      <td>[2, 1353, 1442, 1, 1, 1353, 1362, 1379, 4]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8410 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          word  ...                                      encoded_words\n",
              "6463                              [خ, ن, ث, ی]  ...                     [2, 1359, 1377, 1356, 1442, 4]\n",
              "4758                           [ت, م, س, ا, ح]  ...               [2, 1355, 1376, 1364, 1352, 1358, 4]\n",
              "6501   [خ, ی, ا, ل,  , پ, ر, د, ا, ز, ا, ن, ه]  ...  [2, 1359, 1442, 1352, 1375, 1, 1407, 1362, 136...\n",
              "2491                        [ب, د,  , ذ, ا, ت]  ...            [2, 1353, 1360, 1, 1361, 1352, 1355, 4]\n",
              "14262                       [ه, م, س, ا, ز, ی]  ...         [2, 1378, 1376, 1364, 1352, 1363, 1442, 4]\n",
              "...                                        ...  ...                                                ...\n",
              "10299     [ف, ا, ر, غ, ‌, ا, ل, ت, ح, ص, ی, ل]  ...  [2, 1373, 1352, 1362, 1371, 1, 1352, 1375, 135...\n",
              "8473                              [ص, ن, ع, ت]  ...                     [2, 1366, 1377, 1370, 1355, 4]\n",
              "1766                           [پ, و, ش, ا, ک]  ...               [2, 1407, 1379, 1365, 1352, 1426, 4]\n",
              "11765                    [و, ر, ج, ا, و, ن, د]  ...   [2, 1379, 1362, 1357, 1352, 1379, 1377, 1360, 4]\n",
              "14801                    [ب, ی, ‌, آ, ب, ر, و]  ...         [2, 1353, 1442, 1, 1, 1353, 1362, 1379, 4]\n",
              "\n",
              "[8410 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxNM99aDlLir"
      },
      "source": [
        "#join(root_path,\"dataframee.xlsx\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5B8_5R7XkVtd"
      },
      "source": [
        "#writer = pd.ExcelWriter(join(root_path,\"dataframee2.xlsx\"))\n",
        "#df_data_new.to_excel(writer,encoding='utf8')\n",
        "#writer.save()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbCUIKwkpouB",
        "outputId": "be827fd0-5b5b-4117-edf1-35dd8ccc68d3"
      },
      "source": [
        "data_sequence_lengths = [len(text)\n",
        "                         for text in df_data_new['encoded_words']]\n",
        "max_length = max(data_sequence_lengths)\n",
        "MAX_LEN = max_length + 8\n",
        "MAX_LEN"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "45"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEu3pkT7g0Gf"
      },
      "source": [
        "##  Split data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDI4uKiFX9_a",
        "outputId": "74d156b7-8b5d-4d63-d7d8-393835d5ae28"
      },
      "source": [
        "df_train,df_test = train_test_split(df_data_new)\n",
        "first_sentense = df_train.iloc[0]['word']\n",
        "first_sentense"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['خ', 'ر', 'د', 'س', 'ا', 'ل', 'ی']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5vU9QrXleOS"
      },
      "source": [
        "writer_train = pd.ExcelWriter(join(root_path,\"df_big_train.xlsx\"))\n",
        "writer_test = pd.ExcelWriter(join(root_path,\"df_big_test.xlsx\"))\n",
        "df_train.to_excel(writer_train,encoding='utf8')\n",
        "df_test.to_excel(writer_test,encoding='utf8')\n",
        "writer_train.save()\n",
        "writer_test.save()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPPGAwunZGrJ",
        "outputId": "7a64edc2-b8b2-4a01-d7b7-fcd750ce9bf5"
      },
      "source": [
        "df_train.iloc[0]['encoded_words']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 1359, 1362, 1360, 1364, 1352, 1375, 1442, 4]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BcyTRMAjoqYU",
        "outputId": "d612abbf-7630-45a6-882f-94ff5d269796"
      },
      "source": [
        "def encode_dataset(tokenizer, text_sequences, max_length):\n",
        "    token_ids = np.zeros(shape=(len(text_sequences), max_length),\n",
        "                         dtype=np.int32)\n",
        "    for i, text_sequence in enumerate(text_sequences):\n",
        "        encoded = tokenizer.encode(text_sequence)\n",
        "        token_ids[i, 0:len(encoded)] = encoded\n",
        "    attention_masks = (token_ids != 0).astype(np.int32)\n",
        "    return {\"input_ids\": token_ids, \"attention_masks\": attention_masks}\n",
        "\n",
        "\n",
        "encoded_test = encode_dataset(tokenizer, df_test[\"word\"], MAX_LEN)\n",
        "encoded_train = encode_dataset(tokenizer, df_train[\"word\"], MAX_LEN)\n",
        "encoded_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'attention_masks': array([[1, 1, 1, ..., 0, 0, 0],\n",
              "        [1, 1, 1, ..., 0, 0, 0],\n",
              "        [1, 1, 1, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [1, 1, 1, ..., 0, 0, 0],\n",
              "        [1, 1, 1, ..., 0, 0, 0],\n",
              "        [1, 1, 1, ..., 0, 0, 0]], dtype=int32),\n",
              " 'input_ids': array([[   2, 1359, 1362, ...,    0,    0,    0],\n",
              "        [   2, 1352, 1377, ...,    0,    0,    0],\n",
              "        [   2, 1370, 1361, ...,    0,    0,    0],\n",
              "        ...,\n",
              "        [   2, 1373, 1366, ...,    0,    0,    0],\n",
              "        [   2, 1352, 1377, ...,    0,    0,    0],\n",
              "        [   2, 1362, 1365, ...,    0,    0,    0]], dtype=int32)}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdG3oigkp8L-",
        "outputId": "e8e58a04-9fb8-4f1e-d062-2b695c882ef4"
      },
      "source": [
        "encoded_train[\"attention_masks\"][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDm1ewVcp1uQ",
        "outputId": "b7f34c9b-ffdd-4305-f5fb-e4e428109a2c"
      },
      "source": [
        "test_labels = df_test['label'].values\n",
        "train_labels = df_train['label'].values\n",
        "len(train_labels),train_labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6307, array([0, 1, 1, ..., 1, 0, 1]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kiwUHyZoEKA"
      },
      "source": [
        "## Create torch DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mj7TGjUsYMU",
        "outputId": "61a6021f-5a18-4626-cce9-757e15853689"
      },
      "source": [
        "Batch_size = 32\n",
        "train_labels_tensor = torch.utils.data.DataLoader(train_labels , batch_size=Batch_size , shuffle=False)\n",
        "test_labels_tensor = torch.utils.data.DataLoader(test_labels , batch_size=Batch_size , shuffle=False)\n",
        "for l in enumerate(test_labels_tensor):\n",
        "  print(l[1])\n",
        "  ll = l[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,\n",
            "        1, 0, 0, 0, 1, 0, 1, 0])\n",
            "tensor([0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,\n",
            "        1, 0, 1, 1, 0, 0, 0, 0])\n",
            "tensor([0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,\n",
            "        1, 1, 1, 1, 1, 0, 1, 1])\n",
            "tensor([1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,\n",
            "        0, 1, 0, 1, 0, 1, 1, 1])\n",
            "tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,\n",
            "        1, 0, 0, 0, 1, 0, 1, 0])\n",
            "tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,\n",
            "        1, 1, 1, 0, 1, 0, 0, 0])\n",
            "tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,\n",
            "        1, 1, 1, 1, 0, 1, 1, 0])\n",
            "tensor([0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 1, 0, 1, 1, 1])\n",
            "tensor([0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,\n",
            "        0, 1, 1, 0, 1, 0, 0, 1])\n",
            "tensor([0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,\n",
            "        1, 0, 0, 0, 1, 0, 1, 0])\n",
            "tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,\n",
            "        0, 0, 1, 0, 1, 1, 1, 1])\n",
            "tensor([1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,\n",
            "        1, 0, 0, 1, 0, 0, 0, 0])\n",
            "tensor([1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,\n",
            "        1, 1, 1, 0, 0, 1, 1, 1])\n",
            "tensor([0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,\n",
            "        1, 1, 1, 0, 0, 0, 1, 1])\n",
            "tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,\n",
            "        1, 1, 1, 1, 1, 0, 1, 0])\n",
            "tensor([0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
            "        0, 0, 0, 1, 0, 1, 0, 1])\n",
            "tensor([0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
            "        0, 1, 0, 0, 0, 1, 0, 0])\n",
            "tensor([0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,\n",
            "        1, 0, 0, 1, 1, 1, 0, 1])\n",
            "tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,\n",
            "        1, 1, 1, 0, 1, 1, 0, 1])\n",
            "tensor([0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,\n",
            "        1, 0, 0, 1, 1, 1, 1, 0])\n",
            "tensor([1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,\n",
            "        1, 1, 0, 1, 0, 1, 1, 1])\n",
            "tensor([1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
            "        0, 1, 1, 0, 1, 1, 1, 1])\n",
            "tensor([1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,\n",
            "        0, 0, 1, 0, 1, 1, 1, 0])\n",
            "tensor([0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,\n",
            "        0, 1, 1, 0, 0, 0, 1, 0])\n",
            "tensor([0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,\n",
            "        1, 1, 1, 1, 0, 1, 0, 0])\n",
            "tensor([1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,\n",
            "        0, 1, 1, 0, 0, 1, 1, 1])\n",
            "tensor([1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,\n",
            "        0, 0, 1, 1, 1, 1, 1, 1])\n",
            "tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
            "        1, 1, 1, 0, 1, 1, 1, 1])\n",
            "tensor([1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n",
            "        1, 0, 0, 1, 0, 1, 1, 1])\n",
            "tensor([1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,\n",
            "        0, 1, 0, 0, 0, 1, 0, 1])\n",
            "tensor([1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,\n",
            "        0, 0, 0, 1, 0, 1, 0, 1])\n",
            "tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
            "        0, 1, 1, 1, 1, 0, 1, 0])\n",
            "tensor([1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,\n",
            "        0, 1, 0, 0, 1, 1, 1, 1])\n",
            "tensor([1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,\n",
            "        1, 0, 0, 0, 0, 1, 0, 1])\n",
            "tensor([0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,\n",
            "        0, 1, 0, 1, 0, 0, 0, 0])\n",
            "tensor([1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,\n",
            "        1, 0, 0, 0, 0, 0, 0, 0])\n",
            "tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,\n",
            "        1, 1, 0, 1, 0, 0, 1, 0])\n",
            "tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,\n",
            "        0, 0, 1, 0, 0, 0, 0, 1])\n",
            "tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n",
            "        1, 0, 1, 1, 1, 0, 1, 0])\n",
            "tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,\n",
            "        1, 0, 0, 1, 1, 0, 1, 1])\n",
            "tensor([1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,\n",
            "        0, 0, 1, 1, 1, 0, 0, 0])\n",
            "tensor([1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,\n",
            "        1, 0, 1, 0, 1, 0, 1, 1])\n",
            "tensor([1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
            "        0, 1, 0, 0, 1, 0, 0, 1])\n",
            "tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,\n",
            "        0, 0, 0, 0, 0, 0, 0, 1])\n",
            "tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,\n",
            "        1, 0, 1, 1, 1, 1, 0, 1])\n",
            "tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,\n",
            "        0, 1, 1, 0, 0, 1, 0, 1])\n",
            "tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
            "        0, 1, 1, 1, 1, 0, 0, 0])\n",
            "tensor([0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,\n",
            "        1, 0, 0, 0, 1, 1, 0, 0])\n",
            "tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,\n",
            "        1, 0, 1, 0, 0, 1, 0, 1])\n",
            "tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,\n",
            "        1, 0, 0, 1, 0, 0, 0, 1])\n",
            "tensor([0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,\n",
            "        0, 1, 0, 0, 1, 0, 0, 0])\n",
            "tensor([1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,\n",
            "        0, 1, 1, 1, 1, 0, 0, 0])\n",
            "tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
            "        0, 1, 1, 0, 0, 0, 1, 1])\n",
            "tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
            "        0, 0, 1, 1, 0, 0, 0, 1])\n",
            "tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
            "        1, 1, 1, 0, 1, 1, 1, 1])\n",
            "tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
            "        1, 1, 1, 0, 0, 1, 1, 1])\n",
            "tensor([1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,\n",
            "        0, 0, 1, 0, 0, 1, 0, 0])\n",
            "tensor([1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
            "        0, 0, 1, 0, 1, 0, 0, 1])\n",
            "tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,\n",
            "        0, 0, 1, 1, 0, 1, 1, 1])\n",
            "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
            "        1, 1, 0, 0, 1, 1, 0, 0])\n",
            "tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,\n",
            "        0, 1, 0, 1, 1, 0, 0, 0])\n",
            "tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,\n",
            "        0, 0, 1, 1, 1, 1, 0, 1])\n",
            "tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,\n",
            "        1, 0, 1, 1, 1, 0, 1, 1])\n",
            "tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,\n",
            "        0, 1, 1, 0, 0, 1, 0, 1])\n",
            "tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,\n",
            "        0, 1, 1, 1, 1, 0, 1, 1])\n",
            "tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9sVjQ5ktZ2t",
        "outputId": "d32e906b-0bf8-4eb5-b691-d1678e809f7f"
      },
      "source": [
        "#train_data_tensor_input_ids = torch.tensor(encoded_train['input_ids'])\n",
        "train_data_tensor_input_ids = torch.utils.data.DataLoader(encoded_train['input_ids'], batch_size=Batch_size , shuffle=False)\n",
        "#train_data_tensor_input_ids2 = next(iter(train_data_tensor_input_ids))\n",
        "\n",
        "#train_data_tensor_attention_masks = torch.tensor(encoded_train['attention_masks'])\n",
        "train_data_tensor_attention_masks = torch.utils.data.DataLoader(encoded_train['attention_masks'], batch_size=Batch_size , shuffle=False)\n",
        "#train_data_tensor_attention_masks2 = next(iter(train_data_tensor_attention_masks))\n",
        "\n",
        "#test_data_tensor_input_ids = torch.tensor(encoded_test['input_ids'])\n",
        "test_data_tensor_input_ids = torch.utils.data.DataLoader(encoded_test['input_ids'], batch_size=Batch_size , shuffle=False)\n",
        "#test_data_tensor_input_ids2 = next(iter(test_data_tensor_input_ids))\n",
        "\n",
        "#test_data_tensor_attention_masks = torch.tensor(encoded_test['attention_masks'])\n",
        "test_data_tensor_attention_masks = torch.utils.data.DataLoader(encoded_test['attention_masks'],batch_size=Batch_size , shuffle=False)\n",
        "#test_data_tensor_attention_masks2 = next(iter(test_data_tensor_attention_masks))\n",
        "\n",
        "\n",
        "#test_data_loader = torch.utils.data.DataLoader(encoded_test, 10, True)\n",
        "#test_data_loader2 = next(iter(test_data_tensor_attention_masks))\n",
        "\n",
        "train_data_tensor_input_ids"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7f8cb8940a10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmtL8KPTzOy5",
        "outputId": "c9012eee-8d99-4ada-a2f5-719b4a7f51f9"
      },
      "source": [
        "num_batches = len(df_train)// Batch_size\n",
        "num_batches"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "197"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URsUDAtbm8Jz"
      },
      "source": [
        "just to know, how to iterate through batches :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKfV3yKIU9xu"
      },
      "source": [
        "#for input_ids_batch ,attention_masks_batch in zip(enumerate(test_data_tensor_input_ids),enumerate(test_data_tensor_attention_masks)):\n",
        "#  print(input_ids_batch,attention_masks_batch[1])\n",
        "#  print('============')\n",
        "#  input_ids_batches = input_ids_batch[1]\n",
        "#input_ids_batches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NX9tggI-shx1"
      },
      "source": [
        "# Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdMTUJsKe-59"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cdyel71qrpT8"
      },
      "source": [
        "class RootClassifierModel(nn.Module):\n",
        "    \n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(RootClassifierModel, self).__init__()\n",
        "\n",
        "        self.bert = BertModel.from_pretrained(model_name,return_dict=False)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.Root_classifier = nn.Linear(config.hidden_size, 2 )\n",
        "    \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "    #def forward(self, input):\n",
        "        sequence_output , pooled_output = self.bert(\n",
        "            #input\n",
        "            input_ids=input_ids, \n",
        "            attention_mask=attention_mask\n",
        "            )\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        Root_logits = self.Root_classifier(pooled_output)\n",
        "\n",
        "        return Root_logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9D8jxuMlrpRV",
        "outputId": "03b1d250-c45b-4fe6-cf0c-91b5b8bef536"
      },
      "source": [
        "Root_model = RootClassifierModel(config=config)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at HooshvareLab/bert-fa-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1_Z76O3hLUJ"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "#optimizer = optim.Adam(params=Root_model.parameters(),lr=0.0008)\n",
        "#optimizer = optim.Adam(params=Root_model.parameters())\n",
        "optimizer = optim.Adam(params=Root_model.parameters(), lr=3e-5, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5y2g8UnfDeb"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cg6T7mV7rKz4",
        "outputId": "63e35187-5e22-4643-ed38-e579ee83ee6e"
      },
      "source": [
        "#1\n",
        "#2\n",
        "#3\n",
        "#4\n",
        "num_epoch = 1\n",
        "for epoch in range(num_epoch):\n",
        "  b=0\n",
        "  Root_model.train()\n",
        "\n",
        "  score_list=[]\n",
        "  for input_ids_batch ,attention_masks_batch ,labels_batch in zip(enumerate(train_data_tensor_input_ids),enumerate(train_data_tensor_attention_masks),enumerate(train_labels_tensor)):\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    output = Root_model(input_ids_batch[1],attention_masks_batch[1])\n",
        "    #print(output.shape,labels_batch[1].shape)\n",
        "\n",
        "    loss = criterion(output,labels_batch[1])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    _, preds = torch.max(output, dim=1)\n",
        "    nu=0\n",
        "    for j in range(len(preds)):\n",
        "      if preds[j]==labels_batch[1][j]:\n",
        "        nu+=1\n",
        "    score = (nu/len(preds)) * 100\n",
        "    score_list.append(score)\n",
        "\n",
        "    b+=1\n",
        "    print(f'epoch {epoch+1}/{num_epoch} , batch {b}/{num_batches+1} loss = {loss.item()}')\n",
        "    print(f'batch precision score : {score} % ')\n",
        "    #print(f'precision {precision_score(labels_batch[1],preds)}')\n",
        "    print('----------')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1/1 , batch 1/198 loss = 0.21703565120697021\n",
            "batch precision score : 87.5 % \n",
            "----------\n",
            "epoch 1/1 , batch 2/198 loss = 0.04010931774973869\n",
            "batch precision score : 100.0 % \n",
            "----------\n",
            "epoch 1/1 , batch 3/198 loss = 0.08418770879507065\n",
            "batch precision score : 96.875 % \n",
            "----------\n",
            "epoch 1/1 , batch 4/198 loss = 0.08770041167736053\n",
            "batch precision score : 96.875 % \n",
            "----------\n",
            "epoch 1/1 , batch 5/198 loss = 0.08896567672491074\n",
            "batch precision score : 93.75 % \n",
            "----------\n",
            "epoch 1/1 , batch 6/198 loss = 0.10250198841094971\n",
            "batch precision score : 93.75 % \n",
            "----------\n",
            "epoch 1/1 , batch 7/198 loss = 0.028582777827978134\n",
            "batch precision score : 100.0 % \n",
            "----------\n",
            "epoch 1/1 , batch 8/198 loss = 0.02791781537234783\n",
            "batch precision score : 100.0 % \n",
            "----------\n",
            "epoch 1/1 , batch 9/198 loss = 0.2178158164024353\n",
            "batch precision score : 90.625 % \n",
            "----------\n",
            "epoch 1/1 , batch 10/198 loss = 0.23809072375297546\n",
            "batch precision score : 93.75 % \n",
            "----------\n",
            "epoch 1/1 , batch 11/198 loss = 0.12438087910413742\n",
            "batch precision score : 93.75 % \n",
            "----------\n",
            "epoch 1/1 , batch 12/198 loss = 0.024424981325864792\n",
            "batch precision score : 100.0 % \n",
            "----------\n",
            "epoch 1/1 , batch 13/198 loss = 0.19616863131523132\n",
            "batch precision score : 90.625 % \n",
            "----------\n",
            "epoch 1/1 , batch 14/198 loss = 0.1605750024318695\n",
            "batch precision score : 96.875 % \n",
            "----------\n",
            "epoch 1/1 , batch 15/198 loss = 0.1680881232023239\n",
            "batch precision score : 93.75 % \n",
            "----------\n",
            "epoch 1/1 , batch 16/198 loss = 0.15323513746261597\n",
            "batch precision score : 93.75 % \n",
            "----------\n",
            "epoch 1/1 , batch 17/198 loss = 0.17320704460144043\n",
            "batch precision score : 93.75 % \n",
            "----------\n",
            "epoch 1/1 , batch 18/198 loss = 0.026517877355217934\n",
            "batch precision score : 100.0 % \n",
            "----------\n",
            "epoch 1/1 , batch 19/198 loss = 0.05532611161470413\n",
            "batch precision score : 96.875 % \n",
            "----------\n",
            "epoch 1/1 , batch 20/198 loss = 0.062099166214466095\n",
            "batch precision score : 100.0 % \n",
            "----------\n",
            "epoch 1/1 , batch 21/198 loss = 0.09789847582578659\n",
            "batch precision score : 96.875 % \n",
            "----------\n",
            "epoch 1/1 , batch 22/198 loss = 0.02786277048289776\n",
            "batch precision score : 100.0 % \n",
            "----------\n",
            "epoch 1/1 , batch 23/198 loss = 0.3551911413669586\n",
            "batch precision score : 93.75 % \n",
            "----------\n",
            "epoch 1/1 , batch 24/198 loss = 0.02834746241569519\n",
            "batch precision score : 100.0 % \n",
            "----------\n",
            "epoch 1/1 , batch 25/198 loss = 0.267972856760025\n",
            "batch precision score : 90.625 % \n",
            "----------\n",
            "epoch 1/1 , batch 26/198 loss = 0.06635510176420212\n",
            "batch precision score : 96.875 % \n",
            "----------\n",
            "epoch 1/1 , batch 27/198 loss = 0.22326608002185822\n",
            "batch precision score : 93.75 % \n",
            "----------\n",
            "epoch 1/1 , batch 28/198 loss = 0.12181764841079712\n",
            "batch precision score : 93.75 % \n",
            "----------\n",
            "epoch 1/1 , batch 29/198 loss = 0.25886353850364685\n",
            "batch precision score : 87.5 % \n",
            "----------\n",
            "epoch 1/1 , batch 30/198 loss = 0.11808443069458008\n",
            "batch precision score : 96.875 % \n",
            "----------\n",
            "epoch 1/1 , batch 31/198 loss = 0.1917099952697754\n",
            "batch precision score : 90.625 % \n",
            "----------\n",
            "epoch 1/1 , batch 32/198 loss = 0.07864396274089813\n",
            "batch precision score : 96.875 % \n",
            "----------\n",
            "epoch 1/1 , batch 33/198 loss = 0.10967930406332016\n",
            "batch precision score : 93.75 % \n",
            "----------\n",
            "epoch 1/1 , batch 34/198 loss = 0.12108375132083893\n",
            "batch precision score : 93.75 % \n",
            "----------\n",
            "epoch 1/1 , batch 35/198 loss = 0.19999979436397552\n",
            "batch precision score : 93.75 % \n",
            "----------\n",
            "epoch 1/1 , batch 36/198 loss = 0.1963682621717453\n",
            "batch precision score : 90.625 % \n",
            "----------\n",
            "epoch 1/1 , batch 37/198 loss = 0.31547701358795166\n",
            "batch precision score : 87.5 % \n",
            "----------\n",
            "epoch 1/1 , batch 38/198 loss = 0.11001963168382645\n",
            "batch precision score : 96.875 % \n",
            "----------\n",
            "epoch 1/1 , batch 39/198 loss = 0.17423850297927856\n",
            "batch precision score : 93.75 % \n",
            "----------\n",
            "epoch 1/1 , batch 40/198 loss = 0.08878614753484726\n",
            "batch precision score : 96.875 % \n",
            "----------\n",
            "epoch 1/1 , batch 41/198 loss = 0.14484356343746185\n",
            "batch precision score : 93.75 % \n",
            "----------\n",
            "epoch 1/1 , batch 42/198 loss = 0.17130853235721588\n",
            "batch precision score : 96.875 % \n",
            "----------\n",
            "epoch 1/1 , batch 43/198 loss = 0.03793518245220184\n",
            "batch precision score : 100.0 % \n",
            "----------\n",
            "epoch 1/1 , batch 44/198 loss = 0.09165981411933899\n",
            "batch precision score : 93.75 % \n",
            "----------\n",
            "epoch 1/1 , batch 45/198 loss = 0.17818774282932281\n",
            "batch precision score : 93.75 % \n",
            "----------\n",
            "epoch 1/1 , batch 46/198 loss = 0.08243410289287567\n",
            "batch precision score : 96.875 % \n",
            "----------\n",
            "epoch 1/1 , batch 47/198 loss = 0.26175782084465027\n",
            "batch precision score : 93.75 % \n",
            "----------\n",
            "epoch 1/1 , batch 48/198 loss = 0.16861483454704285\n",
            "batch precision score : 90.625 % \n",
            "----------\n",
            "epoch 1/1 , batch 49/198 loss = 0.14170774817466736\n",
            "batch precision score : 93.75 % \n",
            "----------\n",
            "epoch 1/1 , batch 50/198 loss = 0.362519234418869\n",
            "batch precision score : 81.25 % \n",
            "----------\n",
            "epoch 1/1 , batch 51/198 loss = 0.10963688790798187\n",
            "batch precision score : 96.875 % \n",
            "----------\n",
            "epoch 1/1 , batch 52/198 loss = 0.16069284081459045\n",
            "batch precision score : 93.75 % \n",
            "----------\n",
            "epoch 1/1 , batch 53/198 loss = 0.05125104635953903\n",
            "batch precision score : 100.0 % \n",
            "----------\n",
            "epoch 1/1 , batch 54/198 loss = 0.09679245203733444\n",
            "batch precision score : 93.75 % \n",
            "----------\n",
            "epoch 1/1 , batch 55/198 loss = 0.07657476514577866\n",
            "batch precision score : 96.875 % \n",
            "----------\n",
            "epoch 1/1 , batch 56/198 loss = 0.09258602559566498\n",
            "batch precision score : 96.875 % \n",
            "----------\n",
            "epoch 1/1 , batch 57/198 loss = 0.03365017846226692\n",
            "batch precision score : 100.0 % \n",
            "----------\n",
            "epoch 1/1 , batch 58/198 loss = 0.20628391206264496\n",
            "batch precision score : 90.625 % \n",
            "----------\n",
            "epoch 1/1 , batch 59/198 loss = 0.049143821001052856\n",
            "batch precision score : 96.875 % \n",
            "----------\n",
            "epoch 1/1 , batch 60/198 loss = 0.047316040843725204\n",
            "batch precision score : 100.0 % \n",
            "----------\n",
            "epoch 1/1 , batch 61/198 loss = 0.10108846426010132\n",
            "batch precision score : 93.75 % \n",
            "----------\n",
            "epoch 1/1 , batch 62/198 loss = 0.2530629336833954\n",
            "batch precision score : 90.625 % \n",
            "----------\n",
            "epoch 1/1 , batch 63/198 loss = 0.03314698860049248\n",
            "batch precision score : 100.0 % \n",
            "----------\n",
            "epoch 1/1 , batch 64/198 loss = 0.18681827187538147\n",
            "batch precision score : 90.625 % \n",
            "----------\n",
            "epoch 1/1 , batch 65/198 loss = 0.039642084389925\n",
            "batch precision score : 96.875 % \n",
            "----------\n",
            "epoch 1/1 , batch 66/198 loss = 0.1924760788679123\n",
            "batch precision score : 93.75 % \n",
            "----------\n",
            "epoch 1/1 , batch 67/198 loss = 0.01628374494612217\n",
            "batch precision score : 100.0 % \n",
            "----------\n",
            "epoch 1/1 , batch 68/198 loss = 0.2951294779777527\n",
            "batch precision score : 93.75 % \n",
            "----------\n",
            "epoch 1/1 , batch 69/198 loss = 0.2596519887447357\n",
            "batch precision score : 90.625 % \n",
            "----------\n",
            "epoch 1/1 , batch 70/198 loss = 0.025388235226273537\n",
            "batch precision score : 100.0 % \n",
            "----------\n",
            "epoch 1/1 , batch 71/198 loss = 0.13653060793876648\n",
            "batch precision score : 93.75 % \n",
            "----------\n",
            "epoch 1/1 , batch 72/198 loss = 0.22204312682151794\n",
            "batch precision score : 90.625 % \n",
            "----------\n",
            "epoch 1/1 , batch 73/198 loss = 0.12798140943050385\n",
            "batch precision score : 93.75 % \n",
            "----------\n",
            "epoch 1/1 , batch 74/198 loss = 0.11332936584949493\n",
            "batch precision score : 93.75 % \n",
            "----------\n",
            "epoch 1/1 , batch 75/198 loss = 0.1370978057384491\n",
            "batch precision score : 96.875 % \n",
            "----------\n",
            "epoch 1/1 , batch 76/198 loss = 0.23821187019348145\n",
            "batch precision score : 87.5 % \n",
            "----------\n",
            "epoch 1/1 , batch 77/198 loss = 0.10725042968988419\n",
            "batch precision score : 100.0 % \n",
            "----------\n",
            "epoch 1/1 , batch 78/198 loss = 0.18730051815509796\n",
            "batch precision score : 93.75 % \n",
            "----------\n",
            "epoch 1/1 , batch 79/198 loss = 0.15667809545993805\n",
            "batch precision score : 93.75 % \n",
            "----------\n",
            "epoch 1/1 , batch 80/198 loss = 0.06782219558954239\n",
            "batch precision score : 96.875 % \n",
            "----------\n",
            "epoch 1/1 , batch 81/198 loss = 0.116180419921875\n",
            "batch precision score : 93.75 % \n",
            "----------\n",
            "epoch 1/1 , batch 82/198 loss = 0.19626009464263916\n",
            "batch precision score : 90.625 % \n",
            "----------\n",
            "epoch 1/1 , batch 83/198 loss = 0.1379684954881668\n",
            "batch precision score : 93.75 % \n",
            "----------\n",
            "epoch 1/1 , batch 84/198 loss = 0.11467940360307693\n",
            "batch precision score : 96.875 % \n",
            "----------\n",
            "epoch 1/1 , batch 85/198 loss = 0.04919951781630516\n",
            "batch precision score : 100.0 % \n",
            "----------\n",
            "epoch 1/1 , batch 86/198 loss = 0.21357697248458862\n",
            "batch precision score : 93.75 % \n",
            "----------\n",
            "epoch 1/1 , batch 87/198 loss = 0.09049294888973236\n",
            "batch precision score : 96.875 % \n",
            "----------\n",
            "epoch 1/1 , batch 88/198 loss = 0.11864571273326874\n",
            "batch precision score : 93.75 % \n",
            "----------\n",
            "epoch 1/1 , batch 89/198 loss = 0.061468902975320816\n",
            "batch precision score : 96.875 % \n",
            "----------\n",
            "epoch 1/1 , batch 90/198 loss = 0.2989334166049957\n",
            "batch precision score : 90.625 % \n",
            "----------\n",
            "epoch 1/1 , batch 91/198 loss = 0.12327202409505844\n",
            "batch precision score : 93.75 % \n",
            "----------\n",
            "epoch 1/1 , batch 92/198 loss = 0.020228201523423195\n",
            "batch precision score : 100.0 % \n",
            "----------\n",
            "epoch 1/1 , batch 93/198 loss = 0.037904929369688034\n",
            "batch precision score : 100.0 % \n",
            "----------\n",
            "epoch 1/1 , batch 94/198 loss = 0.057966168969869614\n",
            "batch precision score : 96.875 % \n",
            "----------\n",
            "epoch 1/1 , batch 95/198 loss = 0.11272885650396347\n",
            "batch precision score : 96.875 % \n",
            "----------\n",
            "epoch 1/1 , batch 96/198 loss = 0.061229415237903595\n",
            "batch precision score : 100.0 % \n",
            "----------\n",
            "epoch 1/1 , batch 97/198 loss = 0.1074214056134224\n",
            "batch precision score : 96.875 % \n",
            "----------\n",
            "epoch 1/1 , batch 98/198 loss = 0.14436104893684387\n",
            "batch precision score : 93.75 % \n",
            "----------\n",
            "epoch 1/1 , batch 99/198 loss = 0.1828928142786026\n",
            "batch precision score : 90.625 % \n",
            "----------\n",
            "epoch 1/1 , batch 100/198 loss = 0.07653947174549103\n",
            "batch precision score : 96.875 % \n",
            "----------\n",
            "epoch 1/1 , batch 101/198 loss = 0.20906288921833038\n",
            "batch precision score : 87.5 % \n",
            "----------\n",
            "epoch 1/1 , batch 102/198 loss = 0.030240878462791443\n",
            "batch precision score : 100.0 % \n",
            "----------\n",
            "epoch 1/1 , batch 103/198 loss = 0.07845904678106308\n",
            "batch precision score : 96.875 % \n",
            "----------\n",
            "epoch 1/1 , batch 104/198 loss = 0.08397387713193893\n",
            "batch precision score : 100.0 % \n",
            "----------\n",
            "epoch 1/1 , batch 105/198 loss = 0.014521021395921707\n",
            "batch precision score : 100.0 % \n",
            "----------\n",
            "epoch 1/1 , batch 106/198 loss = 0.08877386897802353\n",
            "batch precision score : 96.875 % \n",
            "----------\n",
            "epoch 1/1 , batch 107/198 loss = 0.13357865810394287\n",
            "batch precision score : 96.875 % \n",
            "----------\n",
            "epoch 1/1 , batch 108/198 loss = 0.08479996770620346\n",
            "batch precision score : 96.875 % \n",
            "----------\n",
            "epoch 1/1 , batch 109/198 loss = 0.15300670266151428\n",
            "batch precision score : 96.875 % \n",
            "----------\n",
            "epoch 1/1 , batch 110/198 loss = 0.010201259516179562\n",
            "batch precision score : 100.0 % \n",
            "----------\n",
            "epoch 1/1 , batch 111/198 loss = 0.09495667368173599\n",
            "batch precision score : 96.875 % \n",
            "----------\n",
            "epoch 1/1 , batch 112/198 loss = 0.02039133943617344\n",
            "batch precision score : 100.0 % \n",
            "----------\n",
            "epoch 1/1 , batch 113/198 loss = 0.2614991068840027\n",
            "batch precision score : 90.625 % \n",
            "----------\n",
            "epoch 1/1 , batch 114/198 loss = 0.046366509050130844\n",
            "batch precision score : 96.875 % \n",
            "----------\n",
            "epoch 1/1 , batch 115/198 loss = 0.09620650857686996\n",
            "batch precision score : 96.875 % \n",
            "----------\n",
            "epoch 1/1 , batch 116/198 loss = 0.08536490052938461\n",
            "batch precision score : 96.875 % \n",
            "----------\n",
            "epoch 1/1 , batch 117/198 loss = 0.03623901680111885\n",
            "batch precision score : 96.875 % \n",
            "----------\n",
            "epoch 1/1 , batch 118/198 loss = 0.09506437182426453\n",
            "batch precision score : 96.875 % \n",
            "----------\n",
            "epoch 1/1 , batch 119/198 loss = 0.028874309733510017\n",
            "batch precision score : 100.0 % \n",
            "----------\n",
            "epoch 1/1 , batch 120/198 loss = 0.2191389948129654\n",
            "batch precision score : 87.5 % \n",
            "----------\n",
            "epoch 1/1 , batch 121/198 loss = 0.14304295182228088\n",
            "batch precision score : 93.75 % \n",
            "----------\n",
            "epoch 1/1 , batch 122/198 loss = 0.0531972199678421\n",
            "batch precision score : 96.875 % \n",
            "----------\n",
            "epoch 1/1 , batch 123/198 loss = 0.1480783224105835\n",
            "batch precision score : 96.875 % \n",
            "----------\n",
            "epoch 1/1 , batch 124/198 loss = 0.13346365094184875\n",
            "batch precision score : 93.75 % \n",
            "----------\n",
            "epoch 1/1 , batch 125/198 loss = 0.1516277939081192\n",
            "batch precision score : 90.625 % \n",
            "----------\n",
            "epoch 1/1 , batch 126/198 loss = 0.11626968532800674\n",
            "batch precision score : 93.75 % \n",
            "----------\n",
            "epoch 1/1 , batch 127/198 loss = 0.24574723839759827\n",
            "batch precision score : 93.75 % \n",
            "----------\n",
            "epoch 1/1 , batch 128/198 loss = 0.11339453607797623\n",
            "batch precision score : 96.875 % \n",
            "----------\n",
            "epoch 1/1 , batch 129/198 loss = 0.04389367252588272\n",
            "batch precision score : 96.875 % \n",
            "----------\n",
            "epoch 1/1 , batch 130/198 loss = 0.10011299699544907\n",
            "batch precision score : 96.875 % \n",
            "----------\n",
            "epoch 1/1 , batch 131/198 loss = 0.10527164489030838\n",
            "batch precision score : 96.875 % \n",
            "----------\n",
            "epoch 1/1 , batch 132/198 loss = 0.1000140830874443\n",
            "batch precision score : 93.75 % \n",
            "----------\n",
            "epoch 1/1 , batch 133/198 loss = 0.08891862630844116\n",
            "batch precision score : 93.75 % \n",
            "----------\n",
            "epoch 1/1 , batch 134/198 loss = 0.15325455367565155\n",
            "batch precision score : 93.75 % \n",
            "----------\n",
            "epoch 1/1 , batch 135/198 loss = 0.16518843173980713\n",
            "batch precision score : 93.75 % \n",
            "----------\n",
            "epoch 1/1 , batch 136/198 loss = 0.09300445020198822\n",
            "batch precision score : 96.875 % \n",
            "----------\n",
            "epoch 1/1 , batch 137/198 loss = 0.07306033372879028\n",
            "batch precision score : 96.875 % \n",
            "----------\n",
            "epoch 1/1 , batch 138/198 loss = 0.12578155100345612\n",
            "batch precision score : 93.75 % \n",
            "----------\n",
            "epoch 1/1 , batch 139/198 loss = 0.08940768241882324\n",
            "batch precision score : 96.875 % \n",
            "----------\n",
            "epoch 1/1 , batch 140/198 loss = 0.16283805668354034\n",
            "batch precision score : 93.75 % \n",
            "----------\n",
            "epoch 1/1 , batch 141/198 loss = 0.06961635500192642\n",
            "batch precision score : 100.0 % \n",
            "----------\n",
            "epoch 1/1 , batch 142/198 loss = 0.2598089575767517\n",
            "batch precision score : 93.75 % \n",
            "----------\n",
            "epoch 1/1 , batch 143/198 loss = 0.0640053004026413\n",
            "batch precision score : 96.875 % \n",
            "----------\n",
            "epoch 1/1 , batch 144/198 loss = 0.2553947865962982\n",
            "batch precision score : 90.625 % \n",
            "----------\n",
            "epoch 1/1 , batch 145/198 loss = 0.2714237570762634\n",
            "batch precision score : 81.25 % \n",
            "----------\n",
            "epoch 1/1 , batch 146/198 loss = 0.06887708604335785\n",
            "batch precision score : 96.875 % \n",
            "----------\n",
            "epoch 1/1 , batch 147/198 loss = 0.07449129968881607\n",
            "batch precision score : 96.875 % \n",
            "----------\n",
            "epoch 1/1 , batch 148/198 loss = 0.15763595700263977\n",
            "batch precision score : 93.75 % \n",
            "----------\n",
            "epoch 1/1 , batch 149/198 loss = 0.09685061872005463\n",
            "batch precision score : 96.875 % \n",
            "----------\n",
            "epoch 1/1 , batch 150/198 loss = 0.0808948501944542\n",
            "batch precision score : 93.75 % \n",
            "----------\n",
            "epoch 1/1 , batch 151/198 loss = 0.02497796341776848\n",
            "batch precision score : 100.0 % \n",
            "----------\n",
            "epoch 1/1 , batch 152/198 loss = 0.048809248954057693\n",
            "batch precision score : 100.0 % \n",
            "----------\n",
            "epoch 1/1 , batch 153/198 loss = 0.13504193723201752\n",
            "batch precision score : 96.875 % \n",
            "----------\n",
            "epoch 1/1 , batch 154/198 loss = 0.2029668390750885\n",
            "batch precision score : 90.625 % \n",
            "----------\n",
            "epoch 1/1 , batch 155/198 loss = 0.2950546443462372\n",
            "batch precision score : 90.625 % \n",
            "----------\n",
            "epoch 1/1 , batch 156/198 loss = 0.10094030201435089\n",
            "batch precision score : 93.75 % \n",
            "----------\n",
            "epoch 1/1 , batch 157/198 loss = 0.033453334122896194\n",
            "batch precision score : 100.0 % \n",
            "----------\n",
            "epoch 1/1 , batch 158/198 loss = 0.06629391759634018\n",
            "batch precision score : 96.875 % \n",
            "----------\n",
            "epoch 1/1 , batch 159/198 loss = 0.0877988263964653\n",
            "batch precision score : 93.75 % \n",
            "----------\n",
            "epoch 1/1 , batch 160/198 loss = 0.11329343169927597\n",
            "batch precision score : 93.75 % \n",
            "----------\n",
            "epoch 1/1 , batch 161/198 loss = 0.11425068974494934\n",
            "batch precision score : 93.75 % \n",
            "----------\n",
            "epoch 1/1 , batch 162/198 loss = 0.27376148104667664\n",
            "batch precision score : 90.625 % \n",
            "----------\n",
            "epoch 1/1 , batch 163/198 loss = 0.14078453183174133\n",
            "batch precision score : 90.625 % \n",
            "----------\n",
            "epoch 1/1 , batch 164/198 loss = 0.09356076270341873\n",
            "batch precision score : 93.75 % \n",
            "----------\n",
            "epoch 1/1 , batch 165/198 loss = 0.025104891508817673\n",
            "batch precision score : 100.0 % \n",
            "----------\n",
            "epoch 1/1 , batch 166/198 loss = 0.1351463794708252\n",
            "batch precision score : 96.875 % \n",
            "----------\n",
            "epoch 1/1 , batch 167/198 loss = 0.11435426026582718\n",
            "batch precision score : 96.875 % \n",
            "----------\n",
            "epoch 1/1 , batch 168/198 loss = 0.040310077369213104\n",
            "batch precision score : 100.0 % \n",
            "----------\n",
            "epoch 1/1 , batch 169/198 loss = 0.21354542672634125\n",
            "batch precision score : 90.625 % \n",
            "----------\n",
            "epoch 1/1 , batch 170/198 loss = 0.11490364372730255\n",
            "batch precision score : 93.75 % \n",
            "----------\n",
            "epoch 1/1 , batch 171/198 loss = 0.0929529145359993\n",
            "batch precision score : 96.875 % \n",
            "----------\n",
            "epoch 1/1 , batch 172/198 loss = 0.10837572067975998\n",
            "batch precision score : 93.75 % \n",
            "----------\n",
            "epoch 1/1 , batch 173/198 loss = 0.11782927811145782\n",
            "batch precision score : 100.0 % \n",
            "----------\n",
            "epoch 1/1 , batch 174/198 loss = 0.10823079198598862\n",
            "batch precision score : 96.875 % \n",
            "----------\n",
            "epoch 1/1 , batch 175/198 loss = 0.13091710209846497\n",
            "batch precision score : 93.75 % \n",
            "----------\n",
            "epoch 1/1 , batch 176/198 loss = 0.12527243793010712\n",
            "batch precision score : 90.625 % \n",
            "----------\n",
            "epoch 1/1 , batch 177/198 loss = 0.3593679368495941\n",
            "batch precision score : 84.375 % \n",
            "----------\n",
            "epoch 1/1 , batch 178/198 loss = 0.06773808598518372\n",
            "batch precision score : 100.0 % \n",
            "----------\n",
            "epoch 1/1 , batch 179/198 loss = 0.11907903850078583\n",
            "batch precision score : 96.875 % \n",
            "----------\n",
            "epoch 1/1 , batch 180/198 loss = 0.09876551479101181\n",
            "batch precision score : 96.875 % \n",
            "----------\n",
            "epoch 1/1 , batch 181/198 loss = 0.04739302024245262\n",
            "batch precision score : 100.0 % \n",
            "----------\n",
            "epoch 1/1 , batch 182/198 loss = 0.05412890762090683\n",
            "batch precision score : 100.0 % \n",
            "----------\n",
            "epoch 1/1 , batch 183/198 loss = 0.22223085165023804\n",
            "batch precision score : 84.375 % \n",
            "----------\n",
            "epoch 1/1 , batch 184/198 loss = 0.10882245004177094\n",
            "batch precision score : 96.875 % \n",
            "----------\n",
            "epoch 1/1 , batch 185/198 loss = 0.1289064586162567\n",
            "batch precision score : 93.75 % \n",
            "----------\n",
            "epoch 1/1 , batch 186/198 loss = 0.02266599051654339\n",
            "batch precision score : 100.0 % \n",
            "----------\n",
            "epoch 1/1 , batch 187/198 loss = 0.33582156896591187\n",
            "batch precision score : 84.375 % \n",
            "----------\n",
            "epoch 1/1 , batch 188/198 loss = 0.07913573831319809\n",
            "batch precision score : 96.875 % \n",
            "----------\n",
            "epoch 1/1 , batch 189/198 loss = 0.12238671630620956\n",
            "batch precision score : 93.75 % \n",
            "----------\n",
            "epoch 1/1 , batch 190/198 loss = 0.11347855627536774\n",
            "batch precision score : 93.75 % \n",
            "----------\n",
            "epoch 1/1 , batch 191/198 loss = 0.021764541044831276\n",
            "batch precision score : 100.0 % \n",
            "----------\n",
            "epoch 1/1 , batch 192/198 loss = 0.0515911802649498\n",
            "batch precision score : 100.0 % \n",
            "----------\n",
            "epoch 1/1 , batch 193/198 loss = 0.24962294101715088\n",
            "batch precision score : 90.625 % \n",
            "----------\n",
            "epoch 1/1 , batch 194/198 loss = 0.12397007644176483\n",
            "batch precision score : 96.875 % \n",
            "----------\n",
            "epoch 1/1 , batch 195/198 loss = 0.16744202375411987\n",
            "batch precision score : 96.875 % \n",
            "----------\n",
            "epoch 1/1 , batch 196/198 loss = 0.20575104653835297\n",
            "batch precision score : 90.625 % \n",
            "----------\n",
            "epoch 1/1 , batch 197/198 loss = 0.13691839575767517\n",
            "batch precision score : 96.875 % \n",
            "----------\n",
            "epoch 1/1 , batch 198/198 loss = 0.09595879912376404\n",
            "batch precision score : 100.0 % \n",
            "----------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xj8Pr3Me7Je"
      },
      "source": [
        "## evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWJPKk4krpK2",
        "outputId": "f8c92603-865e-44e0-e764-867fc6aaf273"
      },
      "source": [
        "i=0\n",
        "num_batches_test = len(df_test)// Batch_size  + 1\n",
        "score_list =[]\n",
        "for input_ids_batch ,attention_masks_batch,labels in zip(enumerate(test_data_tensor_input_ids),enumerate(test_data_tensor_attention_masks),enumerate(test_labels_tensor)):\n",
        "  output2 = Root_model(input_ids_batch[1],attention_masks_batch[1])\n",
        "  _, preds = torch.max(output2, dim=1)\n",
        "  i+=1\n",
        "  nu = 0\n",
        "  for j in range(len(preds)):\n",
        "    if preds[j]==labels[1][j]:\n",
        "      nu+=1\n",
        "  score = (nu/len(preds)) * 100\n",
        "  score_list.append(score)\n",
        "\n",
        "  print(i,nu,'/',len(preds))\n",
        "  print(f'batch precision score : {score} % ')\n",
        "  print('-------')\n",
        "  #if i == 5:\n",
        "  # break\n",
        "total_score = sum(score_list)/len(score_list)\n",
        "print(f'total score is : {total_score}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 30 / 32\n",
            "batch precision score : 93.75 % \n",
            "-------\n",
            "2 29 / 32\n",
            "batch precision score : 90.625 % \n",
            "-------\n",
            "3 27 / 32\n",
            "batch precision score : 84.375 % \n",
            "-------\n",
            "4 31 / 32\n",
            "batch precision score : 96.875 % \n",
            "-------\n",
            "5 30 / 32\n",
            "batch precision score : 93.75 % \n",
            "-------\n",
            "6 31 / 32\n",
            "batch precision score : 96.875 % \n",
            "-------\n",
            "7 30 / 32\n",
            "batch precision score : 93.75 % \n",
            "-------\n",
            "8 31 / 32\n",
            "batch precision score : 96.875 % \n",
            "-------\n",
            "9 30 / 32\n",
            "batch precision score : 93.75 % \n",
            "-------\n",
            "10 28 / 32\n",
            "batch precision score : 87.5 % \n",
            "-------\n",
            "11 27 / 32\n",
            "batch precision score : 84.375 % \n",
            "-------\n",
            "12 29 / 32\n",
            "batch precision score : 90.625 % \n",
            "-------\n",
            "13 30 / 32\n",
            "batch precision score : 93.75 % \n",
            "-------\n",
            "14 32 / 32\n",
            "batch precision score : 100.0 % \n",
            "-------\n",
            "15 29 / 32\n",
            "batch precision score : 90.625 % \n",
            "-------\n",
            "16 31 / 32\n",
            "batch precision score : 96.875 % \n",
            "-------\n",
            "17 30 / 32\n",
            "batch precision score : 93.75 % \n",
            "-------\n",
            "18 26 / 32\n",
            "batch precision score : 81.25 % \n",
            "-------\n",
            "19 28 / 32\n",
            "batch precision score : 87.5 % \n",
            "-------\n",
            "20 29 / 32\n",
            "batch precision score : 90.625 % \n",
            "-------\n",
            "21 26 / 32\n",
            "batch precision score : 81.25 % \n",
            "-------\n",
            "22 30 / 32\n",
            "batch precision score : 93.75 % \n",
            "-------\n",
            "23 30 / 32\n",
            "batch precision score : 93.75 % \n",
            "-------\n",
            "24 31 / 32\n",
            "batch precision score : 96.875 % \n",
            "-------\n",
            "25 29 / 32\n",
            "batch precision score : 90.625 % \n",
            "-------\n",
            "26 31 / 32\n",
            "batch precision score : 96.875 % \n",
            "-------\n",
            "27 30 / 32\n",
            "batch precision score : 93.75 % \n",
            "-------\n",
            "28 28 / 32\n",
            "batch precision score : 87.5 % \n",
            "-------\n",
            "29 30 / 32\n",
            "batch precision score : 93.75 % \n",
            "-------\n",
            "30 29 / 32\n",
            "batch precision score : 90.625 % \n",
            "-------\n",
            "31 31 / 32\n",
            "batch precision score : 96.875 % \n",
            "-------\n",
            "32 29 / 32\n",
            "batch precision score : 90.625 % \n",
            "-------\n",
            "33 30 / 32\n",
            "batch precision score : 93.75 % \n",
            "-------\n",
            "34 30 / 32\n",
            "batch precision score : 93.75 % \n",
            "-------\n",
            "35 32 / 32\n",
            "batch precision score : 100.0 % \n",
            "-------\n",
            "36 32 / 32\n",
            "batch precision score : 100.0 % \n",
            "-------\n",
            "37 26 / 32\n",
            "batch precision score : 81.25 % \n",
            "-------\n",
            "38 29 / 32\n",
            "batch precision score : 90.625 % \n",
            "-------\n",
            "39 30 / 32\n",
            "batch precision score : 93.75 % \n",
            "-------\n",
            "40 28 / 32\n",
            "batch precision score : 87.5 % \n",
            "-------\n",
            "41 31 / 32\n",
            "batch precision score : 96.875 % \n",
            "-------\n",
            "42 31 / 32\n",
            "batch precision score : 96.875 % \n",
            "-------\n",
            "43 29 / 32\n",
            "batch precision score : 90.625 % \n",
            "-------\n",
            "44 29 / 32\n",
            "batch precision score : 90.625 % \n",
            "-------\n",
            "45 30 / 32\n",
            "batch precision score : 93.75 % \n",
            "-------\n",
            "46 30 / 32\n",
            "batch precision score : 93.75 % \n",
            "-------\n",
            "47 28 / 32\n",
            "batch precision score : 87.5 % \n",
            "-------\n",
            "48 30 / 32\n",
            "batch precision score : 93.75 % \n",
            "-------\n",
            "49 28 / 32\n",
            "batch precision score : 87.5 % \n",
            "-------\n",
            "50 29 / 32\n",
            "batch precision score : 90.625 % \n",
            "-------\n",
            "51 30 / 32\n",
            "batch precision score : 93.75 % \n",
            "-------\n",
            "52 31 / 32\n",
            "batch precision score : 96.875 % \n",
            "-------\n",
            "53 30 / 32\n",
            "batch precision score : 93.75 % \n",
            "-------\n",
            "54 28 / 32\n",
            "batch precision score : 87.5 % \n",
            "-------\n",
            "55 27 / 32\n",
            "batch precision score : 84.375 % \n",
            "-------\n",
            "56 31 / 32\n",
            "batch precision score : 96.875 % \n",
            "-------\n",
            "57 28 / 32\n",
            "batch precision score : 87.5 % \n",
            "-------\n",
            "58 29 / 32\n",
            "batch precision score : 90.625 % \n",
            "-------\n",
            "59 27 / 32\n",
            "batch precision score : 84.375 % \n",
            "-------\n",
            "60 31 / 32\n",
            "batch precision score : 96.875 % \n",
            "-------\n",
            "61 30 / 32\n",
            "batch precision score : 93.75 % \n",
            "-------\n",
            "62 24 / 32\n",
            "batch precision score : 75.0 % \n",
            "-------\n",
            "63 31 / 32\n",
            "batch precision score : 96.875 % \n",
            "-------\n",
            "64 29 / 32\n",
            "batch precision score : 90.625 % \n",
            "-------\n",
            "65 31 / 32\n",
            "batch precision score : 96.875 % \n",
            "-------\n",
            "66 19 / 23\n",
            "batch precision score : 82.6086956521739 % \n",
            "-------\n",
            "total score is : 91.73460144927536\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQAKNO43lQdG"
      },
      "source": [
        "#1 => total score is : 89.57509881422925\n",
        "#2 => total score is : 91.63990447957839\n",
        "#3 => total score is : 91.73460144927536"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwC0-ZW9i4ww"
      },
      "source": [
        "### Save model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6MOeBEVro7j"
      },
      "source": [
        "model_save_name = 'RootClassifierModel_bigdataa.pt'\n",
        "#path = F\"/content/drive/MyDrive/uni2_project/{model_save_name}\" \n",
        "path = root_path + model_save_name\n",
        "torch.save(Root_model, path)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}